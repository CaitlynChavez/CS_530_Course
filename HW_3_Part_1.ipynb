{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_3_Part_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CaitlynChavez/CS_530_Course/blob/main/HW_3_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ98a3-5csUq"
      },
      "source": [
        "## Question 1: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_7SVSLwcw1M"
      },
      "source": [
        "In class we discussed the decomposition of the generalization error (i.e., the error on the \"test set\") into the bias, the variance, and the irreducible error. In this exercise, you will prove this decomposition. Let us assume that our data is generated from model $f$ over data $x$, with irreducible error $\\epsilon\\sim N(0,\\sigma^2)$. Hence, the output $y$ satisfies $y=f(x)+\\epsilon$. Again, we term $\\epsilon$ the irreducicble error because that error is built into how the output $y$ is generated from the input $x$. And no matter what we do, we cannot reduce this error. The reason for that is that, in real life, we only have $y$ and $x$, and not $f$. We strive to approximate $f$ with some $\\hat f$, and thus we approximate $y$ with $\\hat y$ that satisfies $\\hat y=\\hat f(x)$. But we cannot do anything about the noise $\\epsilon$ that is built how $y$ is generated. This means that our (squared) generalization error, $E[(y-\\hat y)^2]$, has $\\sigma^2$ as its lower bound. \n",
        "\n",
        "Now prove that this generalization error, $E[(y-\\hat y)^2$, decomposes in the following manner\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "E[(y-\\hat y)^2 = Bias[\\hat f(x)]^2 + Var[\\hat f(x)] + \\sigma^2 \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where,\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "Bias[\\hat f(x)]^2 = (f(x) - E[\\hat f(x)])^2 \\\\\n",
        "Var[\\hat f(x)]^2 = E[(\\hat f(x) - E[\\hat f(x)])^2]\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Hints: \n",
        "\n",
        "1.   Remember, $\\hat y=\\hat f(x)$\n",
        "2.   Adding and then subtracting the same quantity—and in particular $E[\\hat f(x)]^2$—in this case does not change an equality\n",
        "\n",
        "You can compose the proof on paper, take a picture of it, and upload it to Canvas. You can also solve it below using $\\text{LaTeX}$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig5k74Rulbet"
      },
      "source": [
        "*Can enter answer in Latex here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp4UYnNAFm6n"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u-0qrRtJb4d"
      },
      "source": [
        "## Question 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRrumFdPJO02"
      },
      "source": [
        "You are given a dataset which contains over 100 variables (or dimensions) from over 10,000 customers. And you wish to predict their credit score. It contains little missing data. You decide to impute missing values, remove outliers, and z-score your variables as preprocessing steps—the latter is to keep all the variables in the same range. (Z-scoring a variable consists of subtracting its mean and then dividing the result by the variable's standard deviation.) You decide to use multiple linear-regression for the task. \n",
        "\n",
        "The following are the steps of the pipeline. Do you think the pipeline is sound? Explain. If it is not sound in your opinion, propose an alternative pipeline that would be sound.\n",
        "\n",
        "1. Impute missing values and remove outliers\n",
        "2. Z-score all the variables in the dataset\n",
        "3. Partition data using the training and test sets—e.g., using k-fold crossvalidation\n",
        "4. Train the model on the training set and test it on predicting the credit score on the test set\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVYYV2UiJfpw"
      },
      "source": [
        "# Insert your answer here "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec8HcnJoyMab"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK77Le7RBo19"
      },
      "source": [
        "As a reminder, this is part 1 of 2 of the homework for this week. "
      ]
    }
  ]
}